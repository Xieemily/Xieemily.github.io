

<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>Chess AI - </title>

  <meta name="description" content="五子棋AI">
  <meta name="author" content="Xieemily"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "blog",
    
    "url": "https:\/\/xieemily.github.io\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/xieemily.github.io\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/xieemily.github.io\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/xieemily.github.io\/post\/chess\/",
          "name": "Chess ai"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "Xieemily"
  },
  "headline": "Chess AI",
  "description" : "棋盘落子位置检测，蒙特卡洛搜索及强化学习，人工智能课程大作业，人为设定函数的alpha-beta剪枝没来得及做，有点离谱。\n",
  "inLanguage" : "en",
  "wordCount":  1323 ,
  "datePublished" : "2021-12-12T00:00:00\u002b00:00",
  "dateModified" : "2021-12-12T00:00:00\u002b00:00",
  "image" : "https:\/\/xieemily.github.io\/img\/avatar-icon.png",
  "keywords" : [ "c\u002b\u002b, image process, lane detect" ],
  "mainEntityOfPage" : "https:\/\/xieemily.github.io\/post\/chess\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/xieemily.github.io\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/xieemily.github.io\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>


<meta property="og:title" content="Chess AI" />
<meta property="og:description" content="五子棋AI">
<meta property="og:image" content="https://xieemily.github.io/img/avatar-icon.png" />
<meta property="og:url" content="https://xieemily.github.io/post/chess/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="blog" />

  <meta name="twitter:title" content="Chess AI" />
  <meta name="twitter:description" content="五子棋AI">
  <meta name="twitter:image" content="https://xieemily.github.io/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='https://xieemily.github.io/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.133.0">
  <link rel="alternate" href="https://xieemily.github.io/index.xml" type="application/rss+xml" title="blog"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="https://xieemily.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://xieemily.github.io/css/highlight.min.css" /><link rel="stylesheet" href="https://xieemily.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">


  


  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://xieemily.github.io/">blog</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/tags">Tags</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="blog" href="https://xieemily.github.io/">
            <img class="avatar-img" src="https://xieemily.github.io/img/avatar-icon.png" alt="blog" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>Chess AI</h1>
              
              
              
                
                  <h2 class="post-subheading">五子棋AI</h2>
                
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;Posted on December 12, 2021
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;7&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;1323&nbsp;words
  
  
    
      &nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;Xieemily
    
  
  
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p>棋盘落子位置检测，蒙特卡洛搜索及强化学习，人工智能课程大作业，人为设定函数的alpha-beta剪枝没来得及做，有点离谱。</p>
<h2 id="概述">概述：</h2>
<p>主要分为棋盘检测和五子棋AI实现两部分，棋盘检测首先利用opencv进行图像处理，分割为单个棋盘格，再通过CNN对单个棋盘格图片进行分类，得到棋子类别。五子棋AI实现分别用纯蒙特卡洛方法和基于AlphaZero的简化方法实现。</p>
<h2 id="环境">环境：</h2>
<p>python==3.7.4</p>
<p>numpy==1.19.4</p>
<p>torch==1.7.1</p>
<h2 id="实现">实现：</h2>
<h3 id="棋盘检测image_processpy">棋盘检测<code>image_process.py</code>：</h3>
<p>由于可用的训练数据较少， 棋盘检测采取先分割再检测的方法，将棋盘图片分割为单个棋盘格，再对每个棋盘格利用监督学习方法分类为空格、黑棋和白棋。</p>
<h4 id="1棋盘位置检测">1.棋盘位置检测：</h4>
<p>棋盘检测流程为：</p>
<ul>
<li>canny边缘检测</li>
<li>hough直线检测</li>
<li>直线交点聚类得到棋盘角</li>
<li>棋盘矫正和分割</li>
</ul>
<p><strong>canny:</strong></p>
<p><strong>hough:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">def hough_lines(img):
</span></span><span class="line"><span class="cl">    rho, theta, threshold = 2, np.pi/180, 250
</span></span><span class="line"><span class="cl">    lines = cv2.HoughLines(img,rho,theta,threshold)
</span></span><span class="line"><span class="cl">    return lines
</span></span></code></pre></div><p>聚类直线交点：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">def cluster_intersections(points, max_dist=5):
</span></span><span class="line"><span class="cl">    Y = spatial.distance.pdist(points)
</span></span><span class="line"><span class="cl">    Z = clstr.hierarchy.single(Y)
</span></span><span class="line"><span class="cl">    T = clstr.hierarchy.fcluster(Z, max_dist, &#39;distance&#39;)
</span></span><span class="line"><span class="cl">    clusters = defaultdict(list)
</span></span><span class="line"><span class="cl">    for i in range(len(T)):
</span></span><span class="line"><span class="cl">        clusters[T[i]].append(points[i])
</span></span><span class="line"><span class="cl">    clusters = clusters.values()
</span></span><span class="line"><span class="cl">    clusters = map(lambda arr: (np.mean(np.array(arr)[:, 0]), np.mean(np.array(arr)[:, 1])), clusters)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    result = []
</span></span><span class="line"><span class="cl">    for point in clusters:
</span></span><span class="line"><span class="cl">        result.append([point[0], point[1]])
</span></span><span class="line"><span class="cl">    return result
</span></span></code></pre></div><p>取四个角：</p>
<p><img src="https://xieemily.github.io/media/chess/points.jpg" alt=""></p>
<p><strong>图像变换：</strong></p>
<p><img src="https://xieemily.github.io/media/chess/trans.jpg" alt=""></p>
<p>分割后得到19*19幅图片，单个图片如下：
<img src="https://xieemily.github.io/media/chess/B.jpg" alt=""></p>
<p><img src="https://xieemily.github.io/media/chess/B2.jpg" alt=""></p>
<p><img src="https://xieemily.github.io/media/chess/W.jpg" alt=""></p>
<p>此方法受棋盘拍摄角度影响小，可以较好处理如下图的情况</p>
<p><img src="https://xieemily.github.io/media/chess/09.JPG" alt=""></p>
<p>处理后：</p>
<p><img src="https://xieemily.github.io/media/chess/after.JPG" alt=""></p>
<p>对图进行标记，并旋转、翻转来扩展训练集，得到黑、白、空图片分别约300、400、2000张，按 8：1：1 分到 train/val/test文件夹</p>
<h4 id="2监督学习识别棋子netpy">2.监督学习识别棋子<code>net.py</code></h4>
<p>CNN结构：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">class ChessNet(nn.Module):
</span></span><span class="line"><span class="cl">    def __init__(self):
</span></span><span class="line"><span class="cl">        super(ChessNet, self).__init__()
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        # Defining the convolutional layers of the net
</span></span><span class="line"><span class="cl">        self.conv1 = nn.Conv2d(3, 8, kernel_size=5)
</span></span><span class="line"><span class="cl">        self.conv2 = nn.Conv2d(8, 20, kernel_size=5)
</span></span><span class="line"><span class="cl">        self.conv3 = nn.Conv2d(20, 50, kernel_size=5)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        self.dropout1 = nn.Dropout()
</span></span><span class="line"><span class="cl">        self.dropout2 = nn.Dropout()
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        # Defining the fully connected layers of the net
</span></span><span class="line"><span class="cl">        self.fc1 = nn.Linear(4 * 4 * 50, 64)
</span></span><span class="line"><span class="cl">        self.fc2 = nn.Linear(64, 32)
</span></span><span class="line"><span class="cl">        self.fc3 = nn.Linear(32, 3)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def forward(self, x):
</span></span><span class="line"><span class="cl">        x = F.relu(self.conv1(x))
</span></span><span class="line"><span class="cl">        x = F.max_pool2d(x, 2)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        x = F.relu(self.conv2(x))
</span></span><span class="line"><span class="cl">        x = self.dropout1(x)
</span></span><span class="line"><span class="cl">        x = F.max_pool2d(x, 2)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        x = F.relu(self.conv3(x))
</span></span><span class="line"><span class="cl">        x = self.dropout2(x)
</span></span><span class="line"><span class="cl">        x = F.max_pool2d(x, 2)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        x = x.view(-1, 4 * 4 * 50)  # Convert 2d data to 1d
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        x = F.relu(self.fc1(x))
</span></span><span class="line"><span class="cl">        x = F.relu(self.fc2(x))
</span></span><span class="line"><span class="cl">        x = self.fc3(x)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        return x
</span></span></code></pre></div><p>数据集数据规整，训练结果较好：</p>
<p><img src="https://xieemily.github.io/media/chess/res.jpg" alt=""></p>
<p>test set: <img src="https://xieemily.github.io/media/chess/testset.JPG" alt=""></p>
<p>显示几张test set 中图片及其网络对应输出(E-空, W-白, B-黑)：</p>
<p><img src="https://xieemily.github.io/media/chess/test.jpg" alt=""></p>
<p><img src="https://xieemily.github.io/media/chess/test2.jpg" alt=""></p>
<h3 id="chessai">ChessAI</h3>
<p>传统方法通过搜索博弈树和搜索过程中alpha-beta剪枝来实现，人工设置对局势的评估函数。</p>
<p>α值：有或后继的节点，取当前子节点中的最大倒推值为其下界 β值：有与后继的节点，取当前子节点中的最小倒推值为其上界</p>
<p><img src="https://xieemily.github.io/media/chess/image-20201231231741950.png" alt=""></p>
<p>对于不满足α&lt;=N&lt;=β的节点剪枝， 到达搜索深度后即评估局势并返回值</p>
<p>为了方便后续神经网络的应用，此处直接使用纯蒙特卡洛方法</p>
<h4 id="游戏状态表示及棋盘显示界面">游戏状态表示及棋盘显示界面</h4>
<p>**1. 游戏显示</p>
<p>运行游戏： <code>main.py</code></p>
<p><img src="https://xieemily.github.io/media/chess/interface.JPG" alt=""></p>
<p>设置模式：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># play mode
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">USER_VS_USER_MODE = 0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">USER_VS_AI_MODE = 1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">AI_VS_AI_MODE = 2
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">GAME_PLAY_MODE = 1
</span></span></code></pre></div><p>玩家：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">class MAP_ENTRY_TYPE(IntEnum):
</span></span><span class="line"><span class="cl">    MAP_EMPTY = 0,
</span></span><span class="line"><span class="cl">    MAP_PLAYER_ONE = 1,
</span></span><span class="line"><span class="cl">    MAP_PLAYER_TWO = 2
</span></span></code></pre></div><p>显示界面方法定义在 <code>GameMap.py</code>中， <code>map[][]</code>记录棋盘，可通过更改 <code>CHESS_LEN</code>改变棋盘大小</p>
<p><strong>2.游戏状态</strong></p>
<p>游戏状态类定义在 <code>GameState.py</code>，记录棋盘board、前一步x,y、当前玩家turn：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">def __init__(self, board, x, y, turn):
</span></span><span class="line"><span class="cl">        self.board = board
</span></span><span class="line"><span class="cl">        self.x = x
</span></span><span class="line"><span class="cl">        self.y = y
</span></span><span class="line"><span class="cl">        self.result = None
</span></span><span class="line"><span class="cl">        self.turn = turn
</span></span><span class="line"><span class="cl">        if self.turn == MAP_ENTRY_TYPE.MAP_PLAYER_ONE:
</span></span><span class="line"><span class="cl">            self.next_to_move = MAP_ENTRY_TYPE.MAP_PLAYER_TWO
</span></span><span class="line"><span class="cl">        else:
</span></span><span class="line"><span class="cl">            self.next_to_move = MAP_ENTRY_TYPE.MAP_PLAYER_ONE
</span></span></code></pre></div><p><code>game_result()</code>返回游戏结果，函数判断前一步落子点<code>(self.x, self.y)</code>四个方向直线上的9个棋子是否构成5连</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">def game_result(self):
</span></span><span class="line"><span class="cl">    &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        return:
</span></span><span class="line"><span class="cl">         1 if player #1 wins
</span></span><span class="line"><span class="cl">        -1 if player #2 wins
</span></span><span class="line"><span class="cl">         0 if there is a draw
</span></span><span class="line"><span class="cl">         None if result is unknown
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    dir_offset = [(1, 0), (0, 1), (1, 1), (1, -1)]  # direction
</span></span><span class="line"><span class="cl">    line = np.zeros(9)
</span></span><span class="line"><span class="cl">    for i in range(4):  # dir
</span></span><span class="line"><span class="cl">        chess_range = 0
</span></span><span class="line"><span class="cl">        for k in range(-4, 5):  # count 9 position
</span></span><span class="line"><span class="cl">            dx = self.x + k * dir_offset[i][0]
</span></span><span class="line"><span class="cl">            dy = self.y + k * dir_offset[i][1]
</span></span><span class="line"><span class="cl">            if CHESS_LEN &gt; dx &gt;= 0 and CHESS_LEN &gt; dy &gt;= 0:
</span></span><span class="line"><span class="cl">                line[k+4] = self.board[dy][dx]
</span></span><span class="line"><span class="cl">                else:
</span></span><span class="line"><span class="cl">                    line[k+4] = 0
</span></span></code></pre></div><p><code>move(action)</code>返回执行action后的下个状态GameState</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">def move(self, action):
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        consumes action
</span></span><span class="line"><span class="cl">        return:
</span></span><span class="line"><span class="cl">        GameState
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        new_board = copy.deepcopy(self.board)
</span></span><span class="line"><span class="cl">        new_board[action.y][action.x] = self.next_to_move.value
</span></span><span class="line"><span class="cl">        return GameState(new_board, action.x, action.y, action.turn)
</span></span></code></pre></div><p><code>get_valid_moves()</code>返回可用action的list</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">def get_valid_moves(self):
</span></span><span class="line"><span class="cl">    &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        returns list of legal action at current game state
</span></span><span class="line"><span class="cl">        Returns
</span></span><span class="line"><span class="cl">        list of GameAction
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">    indices = np.where(self.board.T == 0)
</span></span><span class="line"><span class="cl">    return [
</span></span><span class="line"><span class="cl">        ChessMove(coords[0], coords[1], self.next_to_move)
</span></span><span class="line"><span class="cl">        for coords in list(zip(indices[0], indices[1]))
</span></span><span class="line"><span class="cl">    ]
</span></span></code></pre></div><h4 id="纯蒙特卡洛方法">纯蒙特卡洛方法</h4>
<p>蒙特卡洛搜索分为三个阶段：</p>
<ul>
<li>Select</li>
<li>Expand</li>
<li>Backup</li>
</ul>
<p>每个节点需记录：</p>
<p>Q , 计算为子节点win - lose</p>
<p>N，访问次数</p>
<p>对于未扩展过的节点进行扩展，对于已扩展的节点，选择高UCT的子节点，到达未访问节点后rollout，纯蒙特卡洛方法采用随机方式rollout，直到到达终止状态，将结果反传并更新路径中节点的Q、V。</p>
<p><img src="https://xieemily.github.io/media/chess/image-20201231190228077.png" alt=""></p>
<p>实现：</p>
<p>树节点类<code>class MonteCarloTreeSearchNode()</code></p>
<p>每个节点保存当前状态state, 访问次数等, 主要成员函数如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">    def q(self):
</span></span><span class="line"><span class="cl">        wins = self._results[self.parent.state.next_to_move]
</span></span><span class="line"><span class="cl">        loses = self._results[-1 * self.parent.state.next_to_move]
</span></span><span class="line"><span class="cl">        return wins - loses
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def n(self):
</span></span><span class="line"><span class="cl">        return self._number_of_visits
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def expand(self):
</span></span><span class="line"><span class="cl">        action = self.untried_actions.pop()
</span></span><span class="line"><span class="cl">        next_state = self.state.move(action)
</span></span><span class="line"><span class="cl">        child_node = TwoPlayersGameMonteCarloTreeSearchNode(
</span></span><span class="line"><span class="cl">            next_state, parent=self
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">        self.children.append(child_node)
</span></span><span class="line"><span class="cl">        return child_node
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def is_terminal_node(self):
</span></span><span class="line"><span class="cl">        return self.state.is_game_over()
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def rollout(self):
</span></span><span class="line"><span class="cl">        current_rollout_state = self.state
</span></span><span class="line"><span class="cl">        while not current_rollout_state.is_game_over():
</span></span><span class="line"><span class="cl">            possible_moves = current_rollout_state.get_legal_actions()
</span></span><span class="line"><span class="cl">            action = self.rollout_policy(possible_moves)
</span></span><span class="line"><span class="cl">            current_rollout_state = current_rollout_state.move(action)
</span></span><span class="line"><span class="cl">        print(&#34;rollout:\n&#34;)
</span></span><span class="line"><span class="cl">        print(current_rollout_state.board)
</span></span><span class="line"><span class="cl">        return current_rollout_state.game_result()
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def backpropagate(self, result):
</span></span><span class="line"><span class="cl">        self._number_of_visits += 1.
</span></span><span class="line"><span class="cl">        self._results[result] += 1.
</span></span><span class="line"><span class="cl">        if self.parent:
</span></span><span class="line"><span class="cl">            self.parent.backpropagate(result)
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">    def best_child(self, c_param=1.4):
</span></span><span class="line"><span class="cl">        choices_weights = [
</span></span><span class="line"><span class="cl">            (c.q / c.n) + c_param * np.sqrt((2 * np.log(self.n) / c.n))
</span></span><span class="line"><span class="cl">            for c in self.children
</span></span><span class="line"><span class="cl">        ]
</span></span><span class="line"><span class="cl">        return self.children[np.argmax(choices_weights)]
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def rollout_policy(self, possible_moves):
</span></span><span class="line"><span class="cl">        return possible_moves[np.random.randint(len(possible_moves))]
</span></span></code></pre></div><p><code>TreeSearch.py</code>:</p>
<p>调用函数<code>beat_action()</code> 进行<code>simulations_number</code>次蒙特卡洛搜索并返回最优策略</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">class MonteCarloTreeSearch(object):
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def __init__(self, board, x, y, turn):
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        node : mctspy.tree.nodes.MonteCarloTreeSearchNode
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        node = TwoPlayersGameMonteCarloTreeSearchNode(GameState(board, x, y, turn))
</span></span><span class="line"><span class="cl">        self.root = node
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def best_action(self, simulations_number):
</span></span><span class="line"><span class="cl">        for _ in range(0, simulations_number):
</span></span><span class="line"><span class="cl">            v = self._tree_policy()
</span></span><span class="line"><span class="cl">            print(&#34;select:\n&#34;)
</span></span><span class="line"><span class="cl">            print(v.state.board)
</span></span><span class="line"><span class="cl">            reward = v.rollout()
</span></span><span class="line"><span class="cl">            print(&#34;result:&#34;)
</span></span><span class="line"><span class="cl">            print(reward)
</span></span><span class="line"><span class="cl">            v.backpropagate(reward)
</span></span><span class="line"><span class="cl">        return self.root.best_child(c_param=0.)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def _tree_policy(self):
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        selects node to run rollout/playout for
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        current_node = self.root
</span></span><span class="line"><span class="cl">        while not current_node.is_terminal_node():
</span></span><span class="line"><span class="cl">            if not current_node.is_fully_expanded():
</span></span><span class="line"><span class="cl">                return current_node.expand()
</span></span><span class="line"><span class="cl">            else:
</span></span><span class="line"><span class="cl">                current_node = current_node.best_child()
</span></span><span class="line"><span class="cl">        return current_node
</span></span></code></pre></div><p><strong>结果：</strong></p>
<p>9*9棋盘</p>
<p>SIMULATION_NUM=1000：</p>
<p><img src="https://xieemily.github.io/media/chess/puremcts.JPG" alt=""></p>
<p>SIMULATION_NUM=2000：
<img src="https://xieemily.github.io/media/chess/puremcts2.JPG" alt=""></p>
<p><img src="https://xieemily.github.io/media/chess/puremcts3.JPG" alt=""></p>
<p>能堵四连的情况，但棋力较弱，且计算时间较长。</p>
<h3 id="结合神经网络">结合神经网络</h3>
<p>基于AlphaZero的结构， 将mcts与policy-value network结合， 通过自我对弈进行强化学习</p>
<p>在rollout阶段不采用随机方式，而是利用神经网络， 神经网络输入棋盘状态s，输出policy和value，指导mcts的选择，mcts模拟的结果（节点状态，概率，结果）作为训练数据训练网络，不断学习。</p>
<p>网络损失函数： $$ l = \sum_t (v_\theta(s_t) - z_t)^2 - \vec{\pi}_t \cdot \log(\vec{p}_\theta(s_t)) $$</p>
<p>训练样本： $$ (s_t, \vec{\pi}_t, z_t) $$</p>
<p>改变上述节点结构，见<code>MonteCarlo.py</code>中<code>class Node</code></p>
<p>存储节点的先验概率：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">class Node:
</span></span><span class="line"><span class="cl">    def __init__(self, prior, game_state):
</span></span><span class="line"><span class="cl">        self.visit_count = 0
</span></span><span class="line"><span class="cl">        self.to_play = game_state.turn
</span></span><span class="line"><span class="cl">        self.prior = prior
</span></span><span class="line"><span class="cl">        self.value_sum = 0
</span></span><span class="line"><span class="cl">        self.children = []
</span></span><span class="line"><span class="cl">        self.game_state = game_state
</span></span></code></pre></div><p>计算UCB： $$ U(s,a) = Q(s,a) + c_{puct}\cdot P(s,a)\cdot\frac{\sqrt{\Sigma_b N(s,b)}}{1+N(s,a)} $$</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">def ucb_score(parent, child):
</span></span><span class="line"><span class="cl">    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)
</span></span><span class="line"><span class="cl">    if child.visit_count &gt; 0:
</span></span><span class="line"><span class="cl">        # The value of the child is from the perspective of the opposing player
</span></span><span class="line"><span class="cl">        value_score = -child.value()
</span></span><span class="line"><span class="cl">    else:
</span></span><span class="line"><span class="cl">        value_score = 0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    return value_score + prior_score
</span></span></code></pre></div><p>模拟后选择下一步移动的策略： $$ \vec{\pi}(s) = N(s, \cdot)^{1/\tau}/\sum_b(N(s,b)^{1/\tau}) $$</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">    def select_action(self, temperature):
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        Select action according to the visit count distribution and the temperature.
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        visit_counts = np.array([child.visit_count for child in self.children])
</span></span><span class="line"><span class="cl">        actions = [child.game_state.action for child in self.children]
</span></span><span class="line"><span class="cl">        if temperature == 0:
</span></span><span class="line"><span class="cl">            action = actions[np.argmax(visit_counts)]
</span></span><span class="line"><span class="cl">        elif temperature == float(&#34;inf&#34;):
</span></span><span class="line"><span class="cl">            action = np.random.choice(actions)
</span></span><span class="line"><span class="cl">        else:
</span></span><span class="line"><span class="cl">            visit_count_distribution = visit_counts ** (1 / temperature)
</span></span><span class="line"><span class="cl">            visit_count_distribution = visit_count_distribution /    sum(visit_count_distribution)
</span></span><span class="line"><span class="cl">            action = np.random.choice(actions, p=visit_count_distribution)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        return action
</span></span></code></pre></div><p>选择子节点时选择最高ucb值子节点：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">    def select_child(self):
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        Select the child with the highest UCB score.
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        best_score = -np.inf
</span></span><span class="line"><span class="cl">        best_action = -1
</span></span><span class="line"><span class="cl">        best_child = None
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        for child in self.children:
</span></span><span class="line"><span class="cl">            score = ucb_score(self, child)
</span></span><span class="line"><span class="cl">            if score &gt; best_score:
</span></span><span class="line"><span class="cl">                best_score = score
</span></span><span class="line"><span class="cl">                best_action = child.game_state.action
</span></span><span class="line"><span class="cl">                best_child = child
</span></span><span class="line"><span class="cl">        return best_action, best_child
</span></span></code></pre></div><p>扩展节点，存在Node的children[]中：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">expand</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">game_state</span><span class="p">,</span> <span class="n">valid_moves</span><span class="p">,</span> <span class="n">action_probs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Expand a node and keep track of the prior policy probability given by neural network
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">game_state</span> <span class="o">=</span> <span class="n">game_state</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">enumerate</span><span class="p">(</span><span class="n">action_probs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="ne">Node</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prob</span><span class="p">,</span> <span class="n">game_state</span><span class="o">=</span><span class="n">game_state</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="n">valid_moves</span><span class="p">[</span><span class="n">a</span><span class="p">])))</span>
</span></span></code></pre></div><p>类MCTS 见<code>MonteCarlo.py</code>：</p>
<p>参数包含状态、网络模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">class MCTS:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def __init__(self, game_state, model):
</span></span><span class="line"><span class="cl">        self.game_state = game_state
</span></span><span class="line"><span class="cl">        self.model = model
</span></span><span class="line"><span class="cl">        self.state = game_state.state
</span></span></code></pre></div><p>主要的运行函数， 根据网络对当前棋盘状态模拟<code>SIMULATION_NUM</code>次，并返回根节点：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">    def run(self):
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        root = Node(0, self.game_state)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        # EXPAND root
</span></span><span class="line"><span class="cl">        action_probs, value = self.model.policy_value_fn(self.game_state)
</span></span><span class="line"><span class="cl">        valid_moves_flatten = self.game_state.valid_moves_flatten()
</span></span><span class="line"><span class="cl">        valid_moves = self.game_state.get_valid_moves()
</span></span><span class="line"><span class="cl">        action_probs = action_probs[np.argwhere(valid_moves_flatten).flatten()]  # remove invalid moves
</span></span><span class="line"><span class="cl">        action_probs /= np.sum(action_probs)
</span></span><span class="line"><span class="cl">        root.expand(self.game_state, valid_moves, action_probs)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        for _ in range(SIMULATION_NUM):
</span></span><span class="line"><span class="cl">            node = root
</span></span><span class="line"><span class="cl">            search_path = [node]
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            # SELECT
</span></span><span class="line"><span class="cl">            while node.expanded():
</span></span><span class="line"><span class="cl">                action, node = node.select_child()
</span></span><span class="line"><span class="cl">                search_path.append(node)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            parent = search_path[-2]
</span></span><span class="line"><span class="cl">            next_game_state = node.game_state
</span></span><span class="line"><span class="cl">            # The value of the new state from the perspective of the other player
</span></span><span class="line"><span class="cl">            value = next_game_state.game_result()
</span></span><span class="line"><span class="cl">            if value is None:
</span></span><span class="line"><span class="cl">                # If the game has not ended:
</span></span><span class="line"><span class="cl">                # EXPAND
</span></span><span class="line"><span class="cl">                action_probs, value = self.model.policy_value_fn(next_game_state)
</span></span><span class="line"><span class="cl">                valid_moves = next_game_state.get_valid_moves()
</span></span><span class="line"><span class="cl">                valid_moves_flatten = next_game_state.valid_moves_flatten()
</span></span><span class="line"><span class="cl">                action_probs = action_probs[np.argwhere(valid_moves_flatten).flatten()]  # remove invalid moves
</span></span><span class="line"><span class="cl">                action_probs /= np.sum(action_probs)
</span></span><span class="line"><span class="cl">                node.expand(next_game_state, valid_moves, action_probs)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            self.backpropagate(search_path, value, parent.game_state.next_to_move * -1)
</span></span><span class="line"><span class="cl">        return root
</span></span></code></pre></div><p>下一步通过自我博弈来生成训练数据，见<code>SelfPlay.py</code></p>
<p>不断根据现有模型生成下一步策略直至一方胜出，记录每一步的棋局状态<code>state[]</code>, 模拟中的概率分布（子节点访问次数比）<code>mcts_prob[]</code>，一方胜出后即得到了对应每个状态的z（对于双方玩家z值相反）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">def self_play_data(model):
</span></span><span class="line"><span class="cl">    states, mcts_probs, current_players = [], [], []
</span></span><span class="line"><span class="cl">    current_player = MAP_ENTRY_TYPE.MAP_PLAYER_ONE
</span></span><span class="line"><span class="cl">    board = np.zeros((CHESS_LEN, CHESS_LEN))
</span></span><span class="line"><span class="cl">    game_state = GameState(board, 0, 0, -1)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    while True:
</span></span><span class="line"><span class="cl">        states.append(game_state.current_state())
</span></span><span class="line"><span class="cl">        current_players.append(game_state.turn)
</span></span><span class="line"><span class="cl">        mcts = MCTS(game_state, model)
</span></span><span class="line"><span class="cl">        root = mcts.run()
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        action_probs = [0 for _ in range(CHESS_LEN*CHESS_LEN)]
</span></span><span class="line"><span class="cl">        for child in root.children:
</span></span><span class="line"><span class="cl">            act = child.game_state.action
</span></span><span class="line"><span class="cl">            action_probs[act.x*CHESS_LEN+act.y] = child.visit_count
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        action_probs = action_probs / np.sum(action_probs)
</span></span><span class="line"><span class="cl">        mcts_probs.append(action_probs)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        action = root.select_action(temperature=0)
</span></span><span class="line"><span class="cl">        game_state = game_state.move(action)
</span></span><span class="line"><span class="cl">        # print(game_state.board)
</span></span><span class="line"><span class="cl">        reward = game_state.game_result()
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        if reward is not None:
</span></span><span class="line"><span class="cl">            winners_z = np.zeros(len(current_players))
</span></span><span class="line"><span class="cl">            if reward != 0:
</span></span><span class="line"><span class="cl">                winners_z[np.array(current_players) == reward] = 1.0
</span></span><span class="line"><span class="cl">                winners_z[np.array(current_players) != reward] = -1.0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            return reward, zip(states, mcts_probs, winners_z)
</span></span></code></pre></div><h4 id="policy-value网络">policy-value网络</h4>
<p>能力有限，时间紧张，网络直接使用了 <a href="https://github.com/junxiaosong/AlphaZero_Gomoku">https://github.com/junxiaosong/AlphaZero_Gomoku</a> 的网络结构。</p>
<p>棋局表示使用了4个8*8的二值特征平面，前两个平面分别表示当前玩家的棋子位置和对手player的棋子位置，第三个平面表示最近一步的落子位置，第四个平面表示的是当前player是不是先手player，如果是先手player则整个平面全部为1，否则全部为0。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">    def current_state(self):
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        the board state from the perspective of the current player.
</span></span><span class="line"><span class="cl">        state shape: 4*width*height
</span></span><span class="line"><span class="cl">        &#34;&#34;&#34;
</span></span><span class="line"><span class="cl">        mat = self.board.T
</span></span><span class="line"><span class="cl">        square_state = np.zeros((4, CHESS_LEN, CHESS_LEN))
</span></span><span class="line"><span class="cl">        square_state[0][mat == self.turn] = 1.0
</span></span><span class="line"><span class="cl">        square_state[1][mat == self.next_to_move] = 1.0
</span></span><span class="line"><span class="cl">        # indicate the last move location
</span></span><span class="line"><span class="cl">        square_state[2][self.x, self.y] = 1.0
</span></span><span class="line"><span class="cl">        if self.turn == MAP_ENTRY_TYPE.MAP_PLAYER_ONE:
</span></span><span class="line"><span class="cl">            square_state[3][:, :] = 1.0  
</span></span><span class="line"><span class="cl">        return square_state[:, ::-1, :]
</span></span></code></pre></div><p>3层公共全卷积网络，使用ReLu激活函数。然后再分成policy和value两个输出，policy端用11的filter进行降维，再接一个全连接层，使用softmax非线性函数直接输出棋盘上每个位置的落子概率；value端用2个1*1的filter进行降维，再接一个64个神经元的全连接层，最后再接一个全连接层，使用tanh非线性函数直接输出[-1,1]之间的局面评分。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">class Net(nn.Module):
</span></span><span class="line"><span class="cl">    &#34;&#34;&#34;policy-value network module&#34;&#34;&#34;
</span></span><span class="line"><span class="cl">    def __init__(self, board_width, board_height):
</span></span><span class="line"><span class="cl">        super(Net, self).__init__()
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        self.board_width = board_width
</span></span><span class="line"><span class="cl">        self.board_height = board_height
</span></span><span class="line"><span class="cl">        # common layers
</span></span><span class="line"><span class="cl">        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)
</span></span><span class="line"><span class="cl">        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
</span></span><span class="line"><span class="cl">        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
</span></span><span class="line"><span class="cl">        # action policy layers
</span></span><span class="line"><span class="cl">        self.act_conv1 = nn.Conv2d(128, 4, kernel_size=1)
</span></span><span class="line"><span class="cl">        self.act_fc1 = nn.Linear(4*board_width*board_height,
</span></span><span class="line"><span class="cl">                                 board_width*board_height)
</span></span><span class="line"><span class="cl">        # state value layers
</span></span><span class="line"><span class="cl">        self.val_conv1 = nn.Conv2d(128, 2, kernel_size=1)
</span></span><span class="line"><span class="cl">        self.val_fc1 = nn.Linear(2*board_width*board_height, 64)
</span></span><span class="line"><span class="cl">        self.val_fc2 = nn.Linear(64, 1)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def forward(self, state_input):
</span></span><span class="line"><span class="cl">        # common layers
</span></span><span class="line"><span class="cl">        x = F.relu(self.conv1(state_input))
</span></span><span class="line"><span class="cl">        x = F.relu(self.conv2(x))
</span></span><span class="line"><span class="cl">        x = F.relu(self.conv3(x))
</span></span><span class="line"><span class="cl">        # action policy layers
</span></span><span class="line"><span class="cl">        x_act = F.relu(self.act_conv1(x))
</span></span><span class="line"><span class="cl">        x_act = x_act.view(-1, 4*self.board_width*self.board_height)
</span></span><span class="line"><span class="cl">        x_act = F.log_softmax(self.act_fc1(x_act))
</span></span><span class="line"><span class="cl">        # state value layers
</span></span><span class="line"><span class="cl">        x_val = F.relu(self.val_conv1(x))
</span></span><span class="line"><span class="cl">        x_val = x_val.view(-1, 2*self.board_width*self.board_height)
</span></span><span class="line"><span class="cl">        x_val = F.relu(self.val_fc1(x_val))
</span></span><span class="line"><span class="cl">        x_val = F.tanh(self.val_fc2(x_val))
</span></span><span class="line"><span class="cl">        return x_act, x_val
</span></span></code></pre></div><p>在训练过程中，保存当前最新模型，self-play数据直接由当前最新模型生成，并用于训练更新自身。但由于计算资源受限，训练速度较慢，8*8棋盘每步模拟400次，经过100局对弈loss从4.8降至3.2左右</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="cl"><span class="n">batch</span> <span class="n">i</span><span class="p">:</span><span class="mi">101</span><span class="p">,</span> <span class="n">episode_len</span><span class="p">:</span><span class="mi">23</span>
</span></span><span class="line"><span class="cl"><span class="n">kl</span><span class="p">:</span><span class="mf">0.00891</span><span class="p">,</span><span class="n">lr_multiplier</span><span class="p">:</span><span class="mf">0.667</span><span class="p">,</span><span class="n">loss</span><span class="p">:</span><span class="mf">3.3900258541107178</span><span class="p">,</span><span class="n">entropy</span><span class="p">:</span><span class="mf">3.2891287803649902</span><span class="p">,</span><span class="n">explained_var_old</span><span class="p">:</span><span class="mf">0.853</span><span class="p">,</span><span class="n">explained_var_new</span><span class="p">:</span><span class="mf">0.874</span>
</span></span><span class="line"><span class="cl"><span class="n">batch</span> <span class="n">i</span><span class="p">:</span><span class="mi">102</span><span class="p">,</span> <span class="n">episode_len</span><span class="p">:</span><span class="mi">31</span>
</span></span><span class="line"><span class="cl"><span class="n">kl</span><span class="p">:</span><span class="mf">0.02935</span><span class="p">,</span><span class="n">lr_multiplier</span><span class="p">:</span><span class="mf">0.667</span><span class="p">,</span><span class="n">loss</span><span class="p">:</span><span class="mf">3.3679325580596924</span><span class="p">,</span><span class="n">entropy</span><span class="p">:</span><span class="mf">3.244642496109009</span><span class="p">,</span><span class="n">explained_var_old</span><span class="p">:</span><span class="mf">0.844</span><span class="p">,</span><span class="n">explained_var_new</span><span class="p">:</span><span class="mf">0.876</span>
</span></span><span class="line"><span class="cl"><span class="n">batch</span> <span class="n">i</span><span class="p">:</span><span class="mi">103</span><span class="p">,</span> <span class="n">episode_len</span><span class="p">:</span><span class="mi">41</span>
</span></span><span class="line"><span class="cl"><span class="n">kl</span><span class="p">:</span><span class="mf">0.03244</span><span class="p">,</span><span class="n">lr_multiplier</span><span class="p">:</span><span class="mf">0.667</span><span class="p">,</span><span class="n">loss</span><span class="p">:</span><span class="mf">3.3015928268432617</span><span class="p">,</span><span class="n">entropy</span><span class="p">:</span><span class="mf">3.254746198654175</span><span class="p">,</span><span class="n">explained_var_old</span><span class="p">:</span><span class="mf">0.883</span><span class="p">,</span><span class="n">explained_var_new</span><span class="p">:</span><span class="mf">0.906</span>
</span></span></code></pre></div><p>本程序中直接使用了现有模型<code>best_policy.model</code></p>
<h3 id="分析">分析</h3>
<p>由于可用棋盘照片数据少，采用了分割后再检测的方式，棋盘检测算法对于单个棋子的检测效果好，但同时检测结果受到图像分割情况的影响较大，虽然对当前数据集图像分割效果好，对于不同光照条件的棋盘，图像分割的参数可能需要不断调整。</p>
<p>AI方面，纯蒙特卡洛方法需要较多次数的模拟，运用强化学习方法后，通过自我对弈，AI的水平不断提高，可以得到较好的效果。</p>
<hr>

        
          <div class="blog-tags">
            
              
              <a href="https://xieemily.github.io/tags/c&#43;&#43;/">c&#43;&#43;</a>&nbsp;
            
              
              <a href="https://xieemily.github.io/tags/image-process/">image process</a>&nbsp;
            
              
              <a href="https://xieemily.github.io/tags/lane-detect/">lane detect</a>&nbsp;
            
          </div>
        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fxieemily.github.io%2fpost%2fchess%2f&amp;text=Chess%20AI&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fxieemily.github.io%2fpost%2fchess%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fxieemily.github.io%2fpost%2fchess%2f&amp;title=Chess%20AI" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fxieemily.github.io%2fpost%2fchess%2f&amp;title=Chess%20AI" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fxieemily.github.io%2fpost%2fchess%2f&amp;title=Chess%20AI" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fxieemily.github.io%2fpost%2fchess%2f&amp;description=Chess%20AI" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          
            
          

          
                  <h4 class="see-also">See also</h4>
                  <ul>
                
                
                    <li><a href="/post/lane/">Lane Detect</a></li>
                
              </ul>

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://xieemily.github.io/post/lane/" data-toggle="tooltip" data-placement="top" title="Lane Detect">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="https://xieemily.github.io/post/summary/" data-toggle="tooltip" data-placement="top" title="?">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      
      
      
      
      
        
      

    </div>
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
		
		  <a href="mailto:xie_my_emily@163.com" title="Email me">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
		
		  <a href="https://github.com/Xieemily" title="GitHub">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              <a href="Xieemily.github.io">Xieemily</a>
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2024
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://xieemily.github.io/">blog</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.133.0</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="https://xieemily.github.io/js/main.js"></script>
<script src="https://xieemily.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://xieemily.github.io/js/load-photoswipe.js"></script>










    
  </body>
</html>

